![[Pasted image 20250519194547.png]]

![[Pasted image 20250519204406.png]]
## **1. Основные понятия**
### **1.1 Недообучение (Underfitting)**
**Что это?**  
Модель слишком простая и плохо обучается даже на тренировочных данных. Она не улавливает закономерности.  

**Признаки:**  
- Низкая точность на **тренировочных** данных.  
- Низкая точность на **тестовых** данных.  

**Причины:**  
- Слишком простая модель (например, линейная регрессия для сложных данных).  
- Недостаточно признаков (фичей).  
- Слишком сильная регуляризация (например, большой коэффициент λ в L2-регуляризации).  

---

### **1.2 Переобучение (Overfitting)**
**Что это?**  
Модель слишком сложная и "запоминает" тренировочные данные, включая шум, вместо обобщения.  

**Признаки:**  
- Высокая точность на **тренировочных** данных.  
- Низкая точность на **тестовых** данных (сильное падение качества).  

**Причины:**  
- Слишком сложная модель (например, глубокая нейросеть для простых данных).  
- Мало данных для обучения.  
- Слишком много признаков (фичей).  
- Отсутствие регуляризации.  

---

## **2. Способы борьбы с недообучением**
1. **Усложнение модели:**  
   - Переход от линейной к полиномиальной регрессии.  
   - Использование более сложных алгоритмов (например, Random Forest вместо линейной модели).  

2. **Добавление признаков (Feature Engineering):**  
   - Генерация новых фичей (например, полиномиальные признаки).  
   - Использование более информативных признаков.  

3. **Уменьшение регуляризации:**  
   - Уменьшение коэффициента λ в L1/L2-регуляризации.  

4. **Увеличение времени обучения:**  
   - Для нейросетей — больше эпох.  

---

## **3. Способы борьбы с переобучением**
1. **Регуляризация:**  
   - **L1 (Lasso)** — обнуляет некоторые веса, помогает в отборе признаков.  
   - **L2 (Ridge)** — уменьшает веса, но не обнуляет.  
   - **ElasticNet** — комбинация L1 и L2.  

2. **Упрощение модели:**  
   - Уменьшение глубины дерева (в Decision Tree, Random Forest).  
   - Уменьшение числа слоёв/нейронов (в нейросетях).  

3. **Кросс-валидация (Cross-Validation):**  
   - Разбиение данных на несколько частей и проверка стабильности модели.  

4. **Ранняя остановка (Early Stopping):**  
   - Для градиентного бустинга и нейросетей — остановка, когда ошибка на валидации начинает расти.  

5. **Дропаут (Dropout) — для нейросетей:**  
   - Случайное "выключение" части нейронов во время обучения.  

6. **Увеличение данных (Data Augmentation):**  
   - Добавление искусственных данных (например, поворот изображений в CV).  

7. **Использование ансамблей (Bagging, Boosting):**  
   - **Bagging (Random Forest)** — уменьшает дисперсию.  
   - **Boosting (XGBoost, LightGBM)** — улучшает обобщение.  

---

## **4. Как выбрать правильную модель?**
1. **Разделение данных:**  
   - Train (обучение) / Validation (подбор параметров) / Test (финальная проверка).  
2. **Кривые обучения (Learning Curves):**  
   - Если ошибка на train и validation высокая → **underfitting**.  
   - Если ошибка на train низкая, а на validation высокая → **overfitting**.  
3. **Градиентный спуск и регуляризация:**  
   - Подбор λ (коэффициента регуляризации) через кросс-валидацию.  

---

## **5. Примеры кода (Python)**
### **Регуляризация в линейной регрессии**
```python
from sklearn.linear_model import Ridge, Lasso

# L2-регуляризация (Ridge)
model = Ridge(alpha=0.5)  # alpha = λ
model.fit(X_train, y_train)

# L1-регуляризация (Lasso)
model = Lasso(alpha=0.1)
model.fit(X_train, y_train)
```

### **Ранняя остановка в XGBoost**
```python
from xgboost import XGBClassifier

model = XGBClassifier(
    early_stopping_rounds=10,  # остановка, если 10 итераций без улучшения
    eval_metric="logloss",
    eval_set=[(X_val, y_val)]
)
model.fit(X_train, y_train)
```

---

## **Итог**
| **Проблема**  | **Причины**               | **Способы борьбы**            |
|---------------|---------------------------|-------------------------------|
| **Недообучение** | Слишком простая модель    | Усложнение, больше фичей, меньше регуляризации |
| **Переобучение** | Слишком сложная модель    | Регуляризация, упрощение, dropout, ансамбли |


[[!Вопросы к экзамену]]