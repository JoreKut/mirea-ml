[[!Вопросы к экзамену]]

Отлично! Давай разберём тему **"Нейронные сети"** так, чтобы ты понял основные концепции и мог уверенно отвечать на экзамене.  

## **1. Что такое нейронная сеть?**  
**Нейронная сеть (Neural Network, NN)** — это математическая модель, вдохновлённая биологическими нейронными сетями (мозгом). Она состоит из **искусственных нейронов**, соединённых между собой, и способна обучаться на данных.  

### **Основные применения:**  
- Классификация изображений (например, распознавание котиков vs собачек)  
- Обработка естественного языка (NLP, ChatGPT)  
- Прогнозирование временных рядов (курс акций, погода)  
- Генерация данных (GAN, Stable Diffusion)  

---

## **2. Из чего состоит нейронная сеть?**  
### **(1) Нейрон (искусственный)**  
Аналог биологического нейрона. Принимает входные данные, вычисляет взвешенную сумму и применяет **функцию активации**.  

Формально:  

$y = f\left(\sum_{i=1}^{n} w_i x_i + b \right)$

- \(x_i\) — входные сигналы  
- \(w_i\) — веса (настраиваются при обучении)  
- \(b\) — смещение (bias)  
- \(f\) — функция активации  

### **(2) Функции активации**  
Нужны, чтобы добавить **нелинейность** (иначе сеть — просто линейная модель).  

Популярные функции:  
- **Sigmoid**: \( $\sigma(x) = \frac{1}{1 + e^{-x}}$ \) (для вероятностей)  
- **ReLU**: \( $\text{ReLU}(x) = \max(0, x)$ \) (самая популярная!)  
- **Tanh**: \( $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$ \) (нормализует выход в [-1, 1])  

### **(3) Слои (Layers)**  
- **Входной слой (Input Layer)**: принимает данные (например, пиксели изображения).  
- **Скрытые слои (Hidden Layers)**: промежуточные вычисления.  
- **Выходной слой (Output Layer)**: выдаёт результат (например, класс объекта).  

Чем больше слоёв, тем "глубже" сеть (**Deep Learning**).  

---

## **3. Как обучается нейронная сеть?**  
### **(1) Прямое распространение (Forward Pass)**  
Данные проходят через все слои, и сеть выдаёт предсказание.  

### **(2) Функция потерь (Loss Function)**  
Оценивает, насколько предсказание отличается от истины. Примеры:  
- **MSE (Mean Squared Error)** — для регрессии  
- **Cross-Entropy** — для классификации  

### **(3) Обратное распространение ошибки (Backpropagation)**  
Самый важный шаг! Алгоритм, который **обновляет веса** для минимизации ошибки.  

Как работает:  
1. Считается градиент функции потерь по весам (цепное правило).  
2. Веса обновляются в сторону, противоположную градиенту (**градиентный спуск**).  

Формула обновления весов:  

$w_{new} = w_{old} - \alpha \cdot \frac{\partial L}{\partial w}$

- \($\alpha$\) — скорость обучения (learning rate)  
- \($\frac{\partial L}{\partial w}$\) — градиент  

---

## **4. Виды нейронных сетей**  
### **(1) Полносвязные сети (Fully Connected, Dense)**  
Каждый нейрон соединён со всеми нейронами следующего слоя.  

### **(2) Свёрточные сети (CNN, ConvNet)**  
Используют **свёртки** для работы с изображениями.  
- **Ядро (фильтр)** скользит по изображению и выделяет признаки (например, края).  
- Пуллинг (Pooling) уменьшает размерность (MaxPooling, AvgPooling).  

### **(3) Рекуррентные сети (RNN, LSTM, GRU)**  
Работают с последовательностями (текст, временные ряды).  
- Имеют **память** (скрытое состояние).  
- LSTM решают проблему "исчезающего градиента".  

### **(4) Трансформеры (Transformer)**  
Основа современных NLP-моделей (ChatGPT).  
- Используют **механизм внимания (Attention)**.  
- Нет рекуррентности, только матричные операции.  

---

## **5. Регуляризация и переобучение**  
**Переобучение (Overfitting)** — когда сеть "запоминает" обучающие данные, но плохо работает на новых.  

**Методы борьбы:**  
- **Dropout**: случайное "выключение" нейронов во время обучения.  
- **L1/L2 регуляризация**: штраф за большие веса.  
- **Batch Normalization**: нормализует активации в слоях.  

---

## **6. Практические советы**  
- **Начинайте с простых моделей** (1-2 слоя).  
- **Используйте ReLU** для скрытых слоёв.  
- **Мониторьте потери** (train vs validation).  
- **Попробуйте Adam** вместо обычного градиентного спуска.  

---

## **Пример вопроса с экзамена**  
❓ *"Объясните принцип работы обратного распространения ошибки (Backpropagation) в нейронных сетях."*  

✅ **Ответ:**  
Backpropagation — это алгоритм обучения нейронной сети, который распространяет ошибку от выходного слоя к входному, вычисляя градиенты функции потерь по весам. С помощью градиентного спуска веса обновляются в сторону, уменьшающую ошибку.  
