## **1. Что такое задача регрессии?**  
**Регрессия** — это тип задачи машинного обучения, где цель — предсказать **непрерывную числовую величину** (например, цену, температуру, зарплату) на основе входных признаков.  

### **Примеры регрессии:**  
- Предсказание стоимости квартиры по её площади, локации и количеству комнат.  
- Прогнозирование спроса на товар в зависимости от сезона и рекламного бюджета.  
- Оценка времени доставки заказа на основе расстояния и загруженности дорог.  

---

## **2. Формальная постановка задачи**  
Дано:  
- **Признаки (features)** ( $X = (x_1, x_2, \dots, x_p)$ ) — входные данные (например, площадь квартиры, этаж, район).  
- **Целевая переменная (target)** \( y \) — число, которое нужно предсказать (например, цена квартиры).  

**Требуется:**  
Найти функцию \( f(X) \), которая приближает \( y \) с минимальной ошибкой:  

$y \approx f(X)$

---

## **3. Типы регрессионных моделей**  
В зависимости от вида функции \( f(X) \) регрессия бывает:  

### **1. Линейная регрессия**  

$f(X) = w_0 + w_1 x_1 + w_2 x_2 + \dots + w_p x_p$
 
- \( $w_0, w_1, \dots, w_p$ \) — параметры модели (веса).  
- Пример: цена квартиры линейно зависит от её площади.  

### **2. Полиномиальная регрессия**  

$f(X) = w_0 + w_1 x + w_2 x^2 + \dots + w_k x^k$

- Модель учитывает нелинейные зависимости (например, рост температуры нелинейно зависит от времени).  

### **3. Другие виды регрессии**  
- **Ridge/Lasso** — регуляризованная линейная регрессия.  
- **Деревья решений и ансамбли** (Random Forest, Gradient Boosting) — нелинейные модели.  

---

## **4. Как оценить качество регрессии?**  
Ошибка модели измеряется с помощью **функции потерь (loss function)**

### **Основные метрики:**  
1. **MSE (Mean Squared Error)** — средний квадрат ошибок:  
   
   $MSE = \frac{1}{n} \sum_{i=1}^n (y_i - f(X_i))^2$
   
   - Чем меньше, тем лучше.  
   - Сильно штрафует большие отклонения.  

2. **RMSE (Root MSE)** — корень из MSE:  

   $RMSE = \sqrt{MSE}$

   - Интерпретируется в единицах целевой переменной (например, рубли, градусы).  

3. **MAE (Mean Absolute Error)** — средняя абсолютная ошибка:  
   
   $MAE = \frac{1}{n} \sum_{i=1}^n |y_i - f(X_i)|$
   
   Менее чувствительна к выбросам, чем MSE.  

4. **R² (коэффициент детерминации)** — доля объяснённой дисперсии:  
   $R^2 = 1 - \frac{\sum (y_i - f(X_i))^2}{\sum (y_i - \bar{y})^2}$
   
   - \( $\bar{y}$ \) — среднее значение \( y \).  
   - \( $R^2 = 1$ \) — идеальное предсказание, \( $R^2 < 0$ \) — модель хуже константы.  

---

## **5 Выводы**  
- **Регрессия** предсказывает числа, а не классы (как в классификации).  
- **Линейная регрессия** — простейшая модель \( $y = w_0 + w_1 x_1 + \dots + w_p x_p$ \).  
- **Качество модели** оценивается через MSE, RMSE, MAE, R².  

**Следующий шаг:** разберём **метод наименьших квадратов** для нахождения оптимальных параметров \( w \).


Отлично! Давай разберём тему регрессии, метод наименьших квадратов (МНК) и среднеквадратическую погрешность (MSE) по шагам.  

---
---

## **3. Метод наименьших квадратов (МНК)**  
МНК минимизирует **сумму квадратов ошибок** (разница между предсказанием и истинным значением).  

**Формально:**  

$\text{Найти } w, w_0: \quad \sum_{i=1}^n (y_i - f(X_i))^2 \rightarrow \min$  

### **Как найти оптимальные веса?**  
#### **1. Аналитическое решение (для линейной регрессии)**  
Если записать модель в матричном виде \( $y = Xw + \varepsilon$ \), то оптимальные веса:  

$w = (X^T X)^{-1} X^T y$

(если \( $X^T X$ \) обратима).  

#### **2. Градиентный спуск (для сложных моделей)**  
Если аналитическое решение невозможно, используют численные методы (например, градиентный спуск).  

---

## **4. Среднеквадратическая погрешность (MSE)**  
MSE (Mean Squared Error) — это среднее значение квадратов ошибок:  

$MSE = \frac{1}{n} \sum_{i=1}^n (y_i - f(X_i))^2$
  

**Свойства MSE:**  
- Чем меньше MSE, тем лучше модель.  
- Штрафует большие ошибки сильнее (из-за квадрата).  
- Используется как **функция потерь** в МНК.  

**Пример вычисления:**  

| \( $y_i$ \) (реальное) | \( $f(X_i)$ \) (предсказанное) | Ошибка \( $y_i - f(X_i)$ \) | Квадрат ошибки |
| ---------------------- | ------------------------------ | --------------------------- | -------------- |
| 3                      | 2.5                            | 0.5                         | 0.25           |
| 5                      | 4.8                            | 0.2                         | 0.04           |
| 7                      | 7.1                            | -0.1                        | 0.01           |

$MSE = \frac{0.25 + 0.04 + 0.01}{3} = \frac{0.3}{3} = 0.1$

---

## **Итог**  
- **Регрессия** — предсказание числа.  
- **МНК** — метод подбора модели, минимизирующий сумму квадратов ошибок.  
- **MSE** — метрика качества регрессии (чем меньше, тем лучше).  


[[!Вопросы к экзамену]]