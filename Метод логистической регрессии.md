## **1. Что такое логистическая регрессия?**
Логистическая регрессия — это **алгоритм классификации**, несмотря на название "регрессия". Он предсказывает **вероятность** принадлежности объекта к определённому классу (обычно бинарная классификация: 0 или 1).

**Примеры применения:**
- Определение, болен ли пациент (да/нет).
- Спам-фильтр (спам/не спам).
- Кредитный скоринг (вернёт кредит/не вернёт).

---

## **2. Математическая основа**
### **Сигмоидная функция (логистическая функция)**
Логистическая регрессия использует **сигмоидную функцию** для преобразования линейной комбинации признаков в вероятность:

$\sigma(z) = \frac{1}{1 + e^{-z}}$

где:
- \( $z = w_0 + w_1x_1 + w_2x_2 + \dots + w_nx_n$ \) — линейная комбинация признаков,
- \( w \) — веса модели,
- \( x \) — признаки объекта.

**Свойства сигмоиды:**
- Выход ∈ (0, 1) — можно интерпретировать как вероятность.
- Если \( $z \to +\infty$ \), то \( $\sigma(z) \to 1$ \).
- Если \( $z \to -\infty$ \), то \( $\sigma(z) \to 0$ \).

---

## **3. Как обучается модель?**
### **Функция потерь (Log Loss)**
В отличие от MSE в линейной регрессии, здесь используется **логистическая функция потерь** (кросс-энтропия):

$J(w) = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(\sigma(z_i)) + (1 - y_i) \log(1 - \sigma(z_i)) \right]$

**Почему не MSE?**  
MSE не подходит, потому что приводит к невыпуклой функции потерь, что усложняет оптимизацию.

### **Оптимизация (градиентный спуск)**
Минимизируем \( J(w) \) с помощью **градиентного спуска**:

$w_j := w_j - \alpha \frac{\partial J(w)}{\partial w_j}$

где:
- \( $\alpha$ \) — скорость обучения,
- \( $\frac{\partial J(w)}{\partial w_j} = \frac{1}{N} \sum_{i=1}^{N} (\sigma(z_i) - y_i) x_{ij}$ \) — градиент.

---

## **4. Регуляризация в логистической регрессии**
Чтобы избежать переобучения, добавляют **L1 (Lasso) или L2 (Ridge)** регуляризацию:

- **L1:** \( $J(w) + \lambda \sum |w_j|$ \) (отбирает признаки).
- **L2:** \( $J(w) + \lambda \sum w_j^2$ \) (сглаживает веса).

---

## **5. Оценка качества модели**
### **Метрики для бинарной классификации:**
1. **Accuracy** (доля правильных ответов).
2. **Precision** (точность: сколько из предсказанных "1" действительно "1").
3. **Recall** (полнота: сколько реальных "1" мы нашли).
4. **F1-Score** (среднее гармоническое Precision и Recall).
5. **ROC-AUC** (площадь под ROC-кривой, показывает качество разделения классов).

---

## **6. Плюсы и минусы**
✅ **Плюсы:**
- Простота интерпретации (можно анализировать веса).
- Быстрое обучение.
- Хорошо работает на линейно разделимых данных.

❌ **Минусы:**
- Плохо работает с нелинейными зависимостями.
- Чувствителен к мультиколлинеарности и выбросам.

---

## **7. Пример кода на Python**
```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Загрузка данных
X, y = ... # твои данные

# Разделение на train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Создание и обучение модели
model = LogisticRegression(penalty='l2', C=1.0)  # C = 1/λ
model.fit(X_train, y_train)

# Предсказание
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
```

---

## **8. Вывод**
Логистическая регрессия — это мощный и интерпретируемый метод для **бинарной классификации**, основанный на вероятностном подходе. Важно понимать:
- Как работает сигмоидная функция.
- Как вычисляется функция потерь.
- Как применяется градиентный спуск.
- Какие метрики использовать для оценки.

[[!Вопросы к экзамену]]