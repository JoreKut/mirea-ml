[[!Вопросы к экзамену]]

### **Кластеризация: алгоритм K-средних (K-means)**  
**Преподаватель по машинному обучению объясняет:**  

#### **1. Что такое кластеризация?**  
Кластеризация — это метод **неконтролируемого обучения** (unsupervised learning), цель которого — разбить данные на группы (**кластеры**) так, чтобы:  
- Объекты внутри одного кластера были **похожи** друг на друга.  
- Объекты из разных кластеров — **отличались**.  

**Примеры применения:**  
- Сегментация клиентов.  
- Группировка изображений.  
- Анализ геномных данных.  

---

#### **2. Алгоритм K-средних (K-means)**  
**Идея:** Разделить данные на **K кластеров**, где каждый кластер описывается своим **центроидом** (средней точкой).  

**Шаги алгоритма:**  
1. **Инициализация центроидов**:  
   - Случайно выбираем **K точек** из данных в качестве начальных центроидов.  
   *(Можно использовать улучшенные методы, например, K-means++)*.  

2. **Назначение точек кластерам (E-step)**:  
   - Для каждой точки находим **ближайший центроид** (по расстоянию, обычно Евклидову).  
   - Точка относится к кластеру этого центроида.  

3. **Пересчет центроидов (M-step)**:  
   - Для каждого кластера вычисляем **новый центроид** — среднее всех его точек.  

4. **Проверка сходимости**:  
   - Если центроиды **перестали значительно меняться** или достигнуто максимальное число итераций — останавливаемся.  
   - Иначе — возвращаемся к шагу 2.  

**Формула для расстояния до центроида (Евклидово расстояние):**  

$d(x, c_i) = \sqrt{\sum_{j=1}^{n} (x_j - c_{ij})^2}$

где \($x$\) — точка, \($c_i$\) — центроид \($i$\)-го кластера.  

---

#### **3. Выбор числа кластеров (K)**  
Проблема: K часто неизвестно заранее.  

**Методы подбора K:**  
- **Elbow Method (метод локтя)**:  
  - Строим график зависимости **суммы квадратов расстояний** (WCSS) от числа кластеров.  
  - Выбираем K на "изгибе" графика (где уменьшение WCSS замедляется).  

- **Silhouette Score**:  
  - Оценивает, насколько хорошо точка соответствует своему кластеру.  
  - Оптимальное K — при **максимальном среднем силуэте**.  

---

#### **4. Плюсы и минусы K-means**  
**Плюсы:**  
- Простота и скорость работы.  
- Масштабируемость на большие данные.  

**Минусы:**  
- Чувствителен к **начальным центроидам** (может сходиться к локальным минимумам).  
- Требует задания **K** заранее.  
- Плохо работает с **кластерами сложной формы** (например, "полумесяцы").  
- Чувствителен к выбросам.  

**Решение для выбросов:** Использовать **K-medoids** (например, алгоритм PAM).  

---

#### **5. Пример кода на Python**  
```python
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Данные
X = ...  # Матрица признаков (n_samples, n_features)

# K-means
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(X)
labels = kmeans.labels_
centroids = kmeans.cluster_centers_

# Визуализация
plt.scatter(X[:, 0], X[:, 1], c=labels)
plt.scatter(centroids[:, 0], centroids[:, 1], marker='X', s=200, c='red')
plt.show()
```

---

#### **6. Важные вопросы на экзамене**  
1. **Как работает K-means?**  
   - Ответ: Итеративно обновляет центроиды и назначает точки ближайшим кластерам.  

2. **Как выбрать K?**  
   - Ответ: Метод локтя или силуэт.  

3. **Что делать, если данные разного масштаба?**  
   - Ответ: Нормализовать (StandardScaler).  

4. **Почему K-means может давать плохие результаты?**  
   - Ответ: Из-за сложной формы кластеров или неудачной инициализации.  

---

**Итог:** K-means — базовый, но мощный алгоритм для кластеризации. Понимай его шаги, ограничения и методы оценки!
