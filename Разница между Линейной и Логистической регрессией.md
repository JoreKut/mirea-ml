Да, разница есть, и она принципиальная! Хотя оба метода относятся к регрессионному анализу, они решают **совершенно разные задачи** и имеют разную математическую основу.  

---

## **1. Разница в задачах**
| **Линейная регрессия** | **Логистическая регрессия** |
|------------------------|----------------------------|
| Решает **задачи регрессии** (предсказание непрерывных значений). | Решает **задачи классификации** (предсказание дискретных меток, обычно бинарных). |
| Пример: Прогнозирование цены дома, температуры, зарплаты. | Пример: Определение спама (да/нет), диагностика болезни. |

---

## **2. Разница в математике**
### **Линейная регрессия**  
- **Формула модели**:  
  $y = w_0 + w_1x_1 + w_2x_2 + \dots + w_nx_n + \epsilon$
  
  - \( $y$ \) — непрерывная целевая переменная,  
  - \( $w$ \) — веса,  
  - \( $\epsilon$ \) — ошибка.  

- **Функция потерь**: **MSE (Mean Squared Error)**  
  $J(w) = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y_i})^2$

### **Логистическая регрессия**  
- **Формула модели**:  

  $P(y=1 \mid x) = \sigma(z) = \frac{1}{1 + e^{-(w_0 + w_1x_1 + \dots + w_nx_n)}}$
 
  - \( $\sigma(z)$ \) — сигмоидная функция (выход ∈ (0, 1)),  
  - \( $P(y=1 \mid x)$ \) — вероятность принадлежности к классу 1.  

- **Функция потерь**: **Log Loss (Binary Cross-Entropy)**  

  $J(w) = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(\sigma(z_i)) + (1 - y_i) \log(1 - \sigma(z_i)) \right]$

---

## **3. Разница в интерпретации выходов**
| **Линейная регрессия**                             | **Логистическая регрессия**                       |
| -------------------------------------------------- | ------------------------------------------------- |
| Возвращает **число** (может быть любым: -∞ до +∞). | Возвращает **вероятность** (от 0 до 1).           |
| Пример: \( y = 25.7 \) (цена акции).               | Пример: \( $P(y=1) = 0.87$ \) (87% что это спам). |
|                                                    |                                                   |

---

## **4. Разница в оптимизации**
- **Линейная регрессия** может решаться **аналитически** (через нормальное уравнение) или градиентным спуском.  
- **Логистическая регрессия** требует **итеративной оптимизации** (градиентный спуск, BFGS и др.), так как её функция потерь не имеет closed-form решения.  

---

## **5. Визуализация**
- **Линейная регрессия** строит **прямую линию** в пространстве данных.  
  ![Linear Regression](https://miro.medium.com/v2/resize:fit:1400/1*TGHfz0AFO1tO1s0_5Rjtyw.png)  

- **Логистическая регрессия** строит **S-образную кривую (сигмоиду)**, разделяющую классы.  
  ![Logistic Regression](https://www.saedsayad.com/images/LogReg_1.png)  

---

## **6. Когда что использовать?**
✅ **Линейная регрессия** — если нужно предсказать **число** (например, стоимость, время, температуру).  
✅ **Логистическая регрессия** — если нужно предсказать **вероятность или класс** (например, отток клиентов, диагноз).  

---

## **Вывод**  
| Критерий | Линейная регрессия | Логистическая регрессия |
|----------|--------------------|--------------------------|
| **Тип задачи** | Регрессия | Классификация |
| **Выход** | Непрерывное число | Вероятность (0 до 1) |
| **Функция активации** | Нет | Сигмоида |
| **Функция потерь** | MSE | Log Loss |
| **Оптимизация** | Аналитически / Градиентный спуск | Только итеративные методы |

**Важно:** Несмотря на название, логистическая регрессия — это **классификатор**, а не регрессор!

[[!Вопросы к экзамену]]