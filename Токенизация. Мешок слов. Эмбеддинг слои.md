[[!Вопросы к экзамену]]

## **1. Эмбеддинги в Keras (TensorFlow)**  
В `Keras` есть слой `Embedding`, который можно добавить в нейросеть.  

### **Пример: Создание модели с Embedding Layer**  
```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Flatten, Dense

# Предположим, у нас есть 1000 уникальных слов, и мы хотим векторы размерности 64
vocab_size = 1000
embedding_dim = 64

model = Sequential([
    # Слой эмбеддинга (input_length = макс. длина последовательности)
    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=10),
    
    # Разворачиваем в 1D (если нужно)
    Flatten(),
    
    # Полносвязный слой для классификации
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
print(model.summary())
```
**Пояснение:**  
- `input_dim` — размер словаря (сколько уникальных токенов).  
- `output_dim` — размерность эмбеддинга (например, 64, 128, 300).  
- `input_length` — длина входных последовательностей (например, 10 слов).  

---

### **Пример 2: Обучение Embedding Layer на своих данных**  
```python
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

# Пример данных
texts = ["Я люблю машинное обучение", "Нейросети это круто", "Машинное обучение это будущее"]
labels = [1, 1, 0]  # Метки для классификации

# Токенизация текста
tokenizer = Tokenizer(num_words=50)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

# Паддинг (чтобы все последовательности были одной длины)
data = pad_sequences(sequences, maxlen=5)

# Создаём модель
model = Sequential([
    Embedding(input_dim=50, output_dim=8, input_length=5),
    Flatten(),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(data, np.array(labels), epochs=5)
```
**Вывод:**  
- `Tokenizer` преобразует слова в числа.  
- `pad_sequences` делает все последовательности одинаковой длины.  
- `Embedding` слой обучается вместе с моделью.  

---

## **2. Эмбеддинги в PyTorch**  
В PyTorch эмбеддинги реализуются через `torch.nn.Embedding`.  

### **Пример: Создание и применение Embedding в PyTorch**  
```python
import torch
import torch.nn as nn

# Размер словаря = 1000, размерность эмбеддинга = 64
embedding = nn.Embedding(num_embeddings=1000, embedding_dim=64)

# Пример входных данных (индексы слов)
input_indices = torch.LongTensor([1, 2, 3, 4])  # Например, ["привет", "мир", "это", "тест"]

# Получаем эмбеддинги
output = embedding(input_indices)
print(output.shape)  # torch.Size([4, 64])
```
**Пояснение:**  
- `num_embeddings` — размер словаря.  
- `embedding_dim` — размерность вектора.  
- На вход принимает тензор с индексами, на выходе — тензор с векторами.  

---

### **Пример 2: Embedding в нейросети (PyTorch)**  
```python
class TextClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.fc = nn.Linear(embed_dim, 1)  # Простой классификатор
        
    def forward(self, x):
        x = self.embedding(x)  # (batch_size, seq_len, embed_dim)
        x = x.mean(dim=1)      # Усредняем по последовательности
        return torch.sigmoid(self.fc(x))

# Пример использования
model = TextClassifier(vocab_size=1000, embed_dim=64)
input_data = torch.LongTensor([[1, 2, 3], [4, 5, 6]])  # Batch из 2 последовательностей
output = model(input_data)
print(output)  # Предсказанные вероятности
```

---

## **3. Использование предобученных эмбеддингов (GloVe, FastText)**  
Можно загрузить готовые эмбеддинги (например, GloVe) и использовать их в слое.  

### **Пример в Keras:**  
```python
# Загрузка предобученных эмбеддингов (например, GloVe)
embeddings_index = {}
with open('glove.6B.100d.txt', encoding='utf-8') as f:
    for line in f:
        word, coefs = line.split(maxsplit=1)
        coefs = np.fromstring(coefs, "f", sep=" ")
        embeddings_index[word] = coefs

# Создаём матрицу эмбеддингов для нашего словаря
embedding_matrix = np.zeros((vocab_size, embedding_dim))
for word, i in tokenizer.word_index.items():
    if i < vocab_size:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector

# Используем в Embedding слое с trainable=False
model.layers[0].set_weights([embedding_matrix])
model.layers[0].trainable = False  # Не обучаем эмбеддинги
```

---

## **Вывод**  
- В **Keras** эмбеддинги добавляются через `Embedding()`.  
- В **PyTorch** — через `nn.Embedding()`.  
- Можно обучать с нуля или использовать предобученные (GloVe, Word2Vec).  