## **1. Что такое метрические методы?**
Метрические методы основаны на идее, что **похожие объекты лежат близко друг к другу в пространстве признаков**. Они используют **метрики** (функции расстояния) для определения "близости" объектов.

**Основные свойства:**
- Не строят явную параметрическую модель (non-parametric).
- Работают на основе сравнения с обучающей выборкой.
- Объект может описываться не набором признаков, а непосредственно вектором расстояний до остальных объектов обучающей выборки.
- Чувствительны к выбору метрики.

---

## **2. Метод k-ближайших соседей (k-NN)**
### **Основная идея**
Для классификации нового объекта:
1. Находят **k** ближайших к нему объектов из обучающей выборки.
2. Смотрят на их метки классов.
3. Присваивают объекту класс, который встречается чаще среди этих соседей.

### **Формальное описание**
- **Вход**: обучающая выборка \( (X_1, y_1), ..., (X_n, y_n) \), где \( X_i \) — признаки, \( y_i \) — класс.
- **Задача**: для нового объекта \( X \) определить класс \( y \).
- **Алгоритм**:
  1. Вычислить расстояния от \( X \) до всех \( X_i \) (например, евклидово расстояние).
  2. Выбрать **k** объектов с наименьшими расстояниями.
  3. Определить класс \( y \) как наиболее частый среди этих k соседей.

### **Пример**
Пусть у нас есть данные о двух классах: ● (класс 1) и ▲ (класс 2).  
Новый объект — ★.  
При **k = 3** среди его соседей: 2 ● и 1 ▲ → ★ относят к классу 1.

---

## **3. Выбор параметра k**
- **Малый k (k = 1)**:  
  - Чувствителен к шуму и выбросам (переобучение).  
  - Граница решений очень сложная.  
- **Большой k**:  
  - Сглаживает границы, но может игнорировать локальные особенности (недообучение).  
  - Оптимальное **k** часто подбирают через кросс-валидацию.

P.S. Обычно берут 3-5 соседей (максимум 7).

---

## **4. Функции расстояния (метрики)**
Выбор метрики сильно влияет на результат:
1. **Евклидово расстояние**:  
   $d(X, Y) = \sqrt{\sum_{i=1}^n (X_i - Y_i)^2}$
2. **Манхэттенское расстояние**:  
   $d(X, Y) = \sum_{i=1}^n |X_i - Y_i|$
3. **Косинусное расстояние** (для текстов и векторов):  
   $d(X, Y) = 1 - \frac{X \cdot Y}{||X|| \cdot ||Y||}$ 

---

## **5. Нормализация признаков**
Поскольку k-NN использует расстояния, **разные масштабы признаков могут искажать результат**.  
**Пример**: если один признак в диапазоне 0–1, а другой 0–1000, второй будет доминировать.  
**Решение**:  
- **Мин-макс нормализация**: \( $X' = \frac{X - X_{min}}{X_{max} - X_{min}}$ \)  
- **Стандартизация (z-нормализация)**: \( $X' = \frac{X - \mu}{\sigma}$ \)  

---

## **6. Преимущества и недостатки k-NN**
### **Плюсы**:
- Простота реализации.
- Нет этапа обучения (ленивый алгоритм).
- Хорошо работает, если данных много и они хорошо разделяются.

### **Минусы**:
- Чувствителен к шуму и выбросам.
- Нужно хранить всю выборку.
- Долгий прогноз (для нового объекта нужно рассчитать расстояния до каждого из обучающей выборки)
- Требует подбора **k** и метрики.

---

## **7. Модификации k-NN**
1. **Взвешенный k-NN**: Ближайшие соседи учитываются с весом (например, \( $\frac{1}{d}$ \)).
2. **Адаптивный k-NN**: Число соседей меняется в зависимости от плотности данных.

---

## **8. Пример кода на Python**
```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Загрузка данных
X, y = ...  # Ваши данные

# Нормализация
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Разделение на train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# Обучение k-NN
knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')
knn.fit(X_train, y_train)

# Предсказание
y_pred = knn.predict(X_test)
```

---

## **Итог**
- **k-NN** — простой, но мощный метод для классификации и регрессии.
- Важно правильно выбрать **k**, **метрику** и **нормализовать данные**.
- Хорош для задач с четкой кластеризацией, но не подходит для очень больших данных.

[[!Вопросы к экзамену]]