Отлично! Давай разберём тему **"Обнаружение объектов. Качество обнаружения объектов"** так, чтобы ты понял её настолько хорошо, что сможешь объяснить даже другому студенту.  

---

## **1. Что такое обнаружение объектов?**  
**Обнаружение объектов (Object Detection)** — это задача компьютерного зрения, которая включает:  
- **Классификацию** объекта (что это за объект?).  
- **Локализацию** объекта (где он находится? → bounding box).  

Примеры алгоритмов:  
- Традиционные: **HOG + SVM, Viola-Jones**.  
- Современные (на основе нейросетей): **R-CNN, Fast R-CNN, Faster R-CNN, YOLO, SSD, RetinaNet**.  

---

## **2. Как оценить качество обнаружения?**  
Для оценки используются **метрики**, основанные на **пересечении предсказанных и истинных bounding box'ов**.  

### **2.1. IoU (Intersection over Union)**  
**IoU** — это мера пересечения между предсказанным (`pred`) и истинным (`gt`) bounding box.  

$IoU = \frac{Area(pred \cap gt)}{Area(pred \cup gt)}$

- **IoU = 1** → идеальное совпадение.  
- **IoU = 0** → нет пересечения.  
- Обычно порог **0.5** считается хорошим детектом.  

### **2.2. Precision, Recall и F1-score**  
Как и в классификации, но с учётом IoU:  

| Метрика                  | Формула                                                     | Описание                                                       |
| ------------------------ | ----------------------------------------------------------- | -------------------------------------------------------------- |
| **Precision (точность)** | \($\frac{TP}{TP + FP}$\)                                    | Доля правильно обнаруженных объектов среди всех предсказанных. |
| **Recall (полнота)**     | \($\frac{TP}{TP + FN}$\)                                    | Доля правильно обнаруженных объектов среди всех истинных.      |
| **F1-score**             | $2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$ | Гармоническое среднее Precision и Recall.                      |

Где:  
- **TP (True Positive)** — правильно обнаруженный объект (IoU ≥ порог).  
- **FP (False Positive)** — ложное срабатывание (IoU < порог или лишний bbox).  
- **FN (False Negative)** — пропущенный объект.  

### **2.3. AP (Average Precision) и mAP (mean Average Precision)**  
**AP** — это площадь под **Precision-Recall кривой** для одного класса.  

**mAP** — среднее AP по всем классам.  

- **mAP@0.5** — среднее AP при IoU=0.5.  
- **mAP@[0.5:0.95]** — среднее AP при IoU от 0.5 до 0.95 с шагом 0.05 (более строгая метрика).  

**Чем выше mAP, тем лучше модель!**  

---

## **3. Пример расчёта метрик**  
Допустим, у нас есть:  
- **Истинные bbox'ы**: 3 кошки.  
- **Предсказанные bbox'ы**: 4 (3 правильных, 1 ложный).  

| Объект | IoU | Статус (при пороге 0.5) |
|--------|-----|--------------------------|
| Кошка 1 | 0.8 | TP |
| Кошка 2 | 0.6 | TP |
| Кошка 3 | 0.4 | FN (IoU < 0.5) |
| Лишний bbox | 0.1 | FP |

- **Precision** = TP / (TP + FP) = 2 / (2 + 1) ≈ 0.66  
- **Recall** = TP / (TP + FN) = 2 / (2 + 1) ≈ 0.66  
- **F1-score** ≈ 0.66  

---

## **4. Что влияет на качество обнаружения?**  
1. **Качество модели** (архитектура, обучение).  
2. **Порог IoU** (чем выше, тем строже).  
3. **Баланс между Precision и Recall** (можно жертвовать одним ради другого).  
4. **Количество классов** (чем больше, тем сложнее).  

---

## **5. Вывод**  
- **IoU** — мера качества локализации.  
- **Precision/Recall/F1** — классические метрики.  
- **mAP** — золотой стандарт для сравнения моделей.  